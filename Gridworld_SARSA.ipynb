{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of SARSA solving a gridworld environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as r\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld:\n",
    "\n",
    "    def __init__(self, size = 8, reward = 10):\n",
    "        self.size = size\n",
    "        self.reward = reward\n",
    "\n",
    "        # Create 2d list with filled with zeros\n",
    "        self.grid = [[0] * self.size for i in range(self.size)]\n",
    "\n",
    "        # Current state of agent\n",
    "        self.current_state = None\n",
    "        self.blocked_fields = None\n",
    "        self.goal_idx = None\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "\n",
    "        # Input error correction\n",
    "        if self.size < 5 or isinstance(self.size, int) == False:\n",
    "            print(\"The size entered for the grid is too small or was no integer. Please try an integer bigger than 4.\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            # Create negative fields\n",
    "            for j in range(int(self.size)):\n",
    "                self.grid[r.randint(0, self.size-1)][r.randint(0, self.size-1)] = r.uniform(-1, 0)\n",
    "\n",
    "\n",
    "            # Saves indeces of blocked field\n",
    "            blocked_fields_idx = []\n",
    "            # Create blocked fields\n",
    "            for k in range(int(self.size/1.5)):\n",
    "                blocked_fields = [r.randint(0, self.size-1), r.randint(0, self.size-1)]\n",
    "                blocked_fields_idx.append(blocked_fields)\n",
    "\n",
    "                self.grid[blocked_fields[0]][blocked_fields[1]] = \"X\"\n",
    "\n",
    "            # Save list of blocked fields for step() function\n",
    "            self.blocked_fields = blocked_fields_idx\n",
    "\n",
    "\n",
    "            # initial index of terminal state\n",
    "            goal_idx = [r.randint(0, self.size-1), r.randint(0, self.size-1)]\n",
    "            \n",
    "            # Check adjacent fields to index of terminal state and update until no blocked field is adjacent\n",
    "            while list(map(add, goal_idx, [0, -1])) in blocked_fields_idx or list(map(add, goal_idx, [0, +1])) in blocked_fields_idx or list(map(add, goal_idx, [-1, 0])) in blocked_fields_idx or list(map(add, goal_idx, [+1, 0])) in blocked_fields_idx: \n",
    "                goal_idx = [r.randint(0, self.size-1), r.randint(0, self.size-1)]\n",
    "\n",
    "            # Save index of terminal state for step() function\n",
    "            self.goal_idx = goal_idx\n",
    "\n",
    "            # Create terminal state with positive reward\n",
    "            self.grid[goal_idx[0]][goal_idx[1]] = self.reward\n",
    "               \n",
    "\n",
    "            starting_point_idx = [r.randint(0, self.size-1), r.randint(0, self.size-1)]\n",
    "\n",
    "            # Update starting point until it is a zero value (Not blocked- or terminal field)\n",
    "            while self.grid[starting_point_idx[0]][starting_point_idx[1]] != 0:\n",
    "                starting_point_idx = [r.randint(0, self.size-1), r.randint(0, self.size-1)]\n",
    "\n",
    "            # Set current state to starting state\n",
    "            self.current_state = starting_point_idx\n",
    "\n",
    "            return self.current_state\n",
    "\n",
    "\n",
    "    def step(self, action, new_state):\n",
    "        '''\n",
    "        Moves agent in grid\n",
    "\n",
    "        args:\n",
    "        action(int): How agent should move (0=left, 1=right, 2=up, 3=down)\n",
    "        '''\n",
    "        self.current_state = new_state\n",
    "\n",
    "        # Left\n",
    "        if action == 0:\n",
    "            # Compute result of action\n",
    "            test_state = list(map(add, self.current_state, [0, -1]))\n",
    "            \n",
    "            # New list index is out of bounds (left)\n",
    "            if test_state[1] == -1:\n",
    "                print(\"Wall\")\n",
    "\n",
    "            # New list index is on field marked with \"X\"\n",
    "            elif test_state in self.blocked_fields:\n",
    "                print(\"field blocked\")\n",
    "\n",
    "            # Action lead to terminal state\n",
    "            elif test_state == self.goal_idx:\n",
    "                print(\"Terminal state reached\")\n",
    "                self.current_state = test_state\n",
    "                done = True\n",
    "                # Return reward for step\n",
    "                return self.current_state, self.grid[self.current_state[0]][self.current_state[1]], done, \n",
    "\n",
    "            # Update state (take action)\n",
    "            else:\n",
    "                self.current_state = test_state\n",
    "                done = False\n",
    "                # Return reward for step\n",
    "                return self.current_state, self.grid[self.current_state[0]][self.current_state[1]], done, \n",
    "\n",
    "\n",
    "        # Right\n",
    "        if action == 1:\n",
    "            # Compute result of action\n",
    "            test_state = list(map(add, self.current_state, [0, +1]))\n",
    "            \n",
    "            # New list index is out of bounds (right)\n",
    "            if test_state[1] == self.size:\n",
    "                print(\"Wall\")\n",
    "\n",
    "            # New list index is on field marked with \"X\"\n",
    "            elif test_state in self.blocked_fields:\n",
    "                print(\"field blocked\")\n",
    "\n",
    "            # Action lead to terminal state\n",
    "            elif test_state == self.goal_idx:\n",
    "                print(\"Terminal state reached\")\n",
    "                self.current_state = test_state\n",
    "                done = True\n",
    "                # Return reward for step\n",
    "                return self.current_state, self.grid[self.current_state[0]][self.current_state[1]], done, \n",
    "\n",
    "            # Update state (take action)\n",
    "            else:\n",
    "                self.current_state = test_state\n",
    "                done = False\n",
    "                # Return reward for step\n",
    "                return self.current_state, self.grid[self.current_state[0]][self.current_state[1]], done, \n",
    "\n",
    "\n",
    "        # Up\n",
    "        if action == 2:\n",
    "            # Compute result of action\n",
    "            test_state = list(map(add, self.current_state, [-1, 0]))\n",
    "            \n",
    "            # New list index is out of bounds (right)\n",
    "            if test_state[0] == -1:\n",
    "                print(\"Wall\")\n",
    "\n",
    "            # New list index is on field marked with \"X\"\n",
    "            elif test_state in self.blocked_fields:\n",
    "                print(\"field blocked\")\n",
    "\n",
    "            # Action lead to terminal state\n",
    "            elif test_state == self.goal_idx:\n",
    "                print(\"Terminal state reached\")\n",
    "                self.current_state = test_state\n",
    "                done = True\n",
    "                # Return reward for step\n",
    "                return self.current_state, self.grid[self.current_state[0]][self.current_state[1]], done, \n",
    "\n",
    "            # Update state (take action)\n",
    "            else:\n",
    "                self.current_state = test_state\n",
    "                done = False\n",
    "                # Return reward for step\n",
    "                return self.current_state, self.grid[self.current_state[0]][self.current_state[1]], done, \n",
    "\n",
    "\n",
    "        # down\n",
    "        if action == 3:\n",
    "            # Compute result of action\n",
    "            test_state = list(map(add, self.current_state, [+1, 0]))\n",
    "            \n",
    "            # New list index is out of bounds (right)\n",
    "            if test_state[0] == self.size:\n",
    "                print(\"Wall\")\n",
    "\n",
    "            # New list index is on field marked with \"X\"\n",
    "            elif test_state in self.blocked_fields:\n",
    "                print(\"field blocked\")\n",
    "\n",
    "            # Action lead to terminal state\n",
    "            elif test_state == self.goal_idx:\n",
    "                print(\"Terminal state reached\")\n",
    "                self.current_state = test_state\n",
    "                done = True\n",
    "                # Return reward for step\n",
    "                return self.current_state, self.grid[self.current_state[0]][self.current_state[1]], done, \n",
    "\n",
    "            # Update state (take action)\n",
    "            else:\n",
    "                self.current_state = test_state\n",
    "                done = False\n",
    "                # Return reward for step\n",
    "                return self.current_state, self.grid[self.current_state[0]][self.current_state[1]], done, \n",
    "\n",
    "        print(self.current_state)\n",
    "\n",
    "\n",
    "    def visualise(self):\n",
    "        # Print in matrix format - Rest of program still uses lists\n",
    "        print(np.matrix(self.grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "epsilon = 0.9\n",
    "total_episodes = 10000\n",
    "max_steps = 100\n",
    "alpha = 0.85\n",
    "gamma = 0.95\n",
    "\n",
    "#Initializing the reward\n",
    "reward=0\n",
    "\n",
    "state_space = 10\n",
    "action_space = 4\n",
    "grid = Gridworld(10)\n",
    "\n",
    "q_values = np.zeros(((state_space*state_space), action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function to learn the Q-value\n",
    "def update(state, state2, reward, action, action2):\n",
    "    predict = q_values[state, action]\n",
    "    target = reward + gamma * q_values[state2, action2]\n",
    "    q_values[state, action] = q_values[state, action] + alpha * (target - predict)\n",
    "\n",
    "\n",
    "\n",
    "# Starting the SARSA learning\n",
    "for episode in range(total_episodes):\n",
    "    t = 0\n",
    "    state = grid.reset()\n",
    "    action = np.argmax(q_values[state, :])\n",
    " \n",
    "    while t < max_steps:\n",
    "         \n",
    "        #Getting the next state\n",
    "        new_state, reward, done = grid.step(action)\n",
    " \n",
    "        #Choosing the next action\n",
    "        new_action = np.argmax[new_state]\n",
    "         \n",
    "        #Learning the Q-value\n",
    "        update(state, new_state, reward, action, new_action)\n",
    " \n",
    "        state = new_state\n",
    "        action = new_action\n",
    "         \n",
    "        #Updating the respective vaLues\n",
    "        t += 1\n",
    "        #reward += 1\n",
    "         \n",
    "        #If at the end of learning process\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f1073c099ddc7feaa79a7ad20594747d23b588351da1e73dea2959a263903ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('iannwtf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

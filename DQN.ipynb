{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dtype\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.storage = np.empty(0)\n",
    "        self.size = self.storage.size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save(self, data):\n",
    "\n",
    "        # Free thousand slots in buffer\n",
    "        while self.storage.size > 9000:\n",
    "            self.storage = np.delete(self.storage, -1)\n",
    "\n",
    "\n",
    "        self.storage = np.append(self.storage, data)\n",
    "        self.storage = np.reshape(self.storage, (-1, 3))\n",
    "\n",
    "\n",
    "        #return self.storage[0]\n",
    "\n",
    "    def sample(self):\n",
    "        return(tf.convert_to_tensor(self.storage[random.randint(0, self.size)], dtype=float))\n",
    "\n",
    "    def lol(self):\n",
    "        print(self.storage[0])\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "\n",
    "print(a)\n",
    "\n",
    "print(type(a.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_network(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.l_1 = tf.keras.layers.Dense(128,activation='relu')\n",
    "    self.l_2 = tf.keras.layers.Dense(128,activation='relu')\n",
    "    self.out = tf.keras.layers.Dense(4, activation='softmax')\n",
    "\n",
    "  def call(self, input_data):\n",
    "    x = self.appr(input_data)\n",
    "    action = self.out(x)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = Q_network()\n",
    "buffer = Buffer()\n",
    "env = gym.make('LunarLander-v2')\n",
    "epsilon = 0.02\n",
    "gamma = 0.01\n",
    "optimizer = tf.keras.optimizers.adam\n",
    "\n",
    "\n",
    "\n",
    "for n in range(50):\n",
    "\n",
    "        observation = env.reset()\n",
    "\n",
    "        for step in range(100):\n",
    "            # Starts environment again\n",
    "            #env.render()\n",
    "\n",
    "            # Choose random action of complete action space\n",
    "            action = env.action_space.sample()\n",
    "            # Do action and save information about its effect\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "\n",
    "            buffer.save(np.array([observation, reward, done], dtype=object))\n",
    "\n",
    "            if done:\n",
    "                #print(\"Episode finished after {} timesteps\".format(step + 1))\n",
    "                break\n",
    "        env.close()\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "                # Obtain q-value prediction\n",
    "\n",
    "                q_values_pred = q_network(buffer.sample())\n",
    "                #print(q_values_pred)\n",
    "\n",
    "                # Epsilon greedy\n",
    "                sample_epsilon = np.random.rand()\n",
    "                \n",
    "                # Random action\n",
    "                if sample_epsilon <= epsilon:\n",
    "                        action = np.random.choice(4)\n",
    "\n",
    "                else:\n",
    "                        action = np.argmax(q_values_pred[0])\n",
    "\n",
    "        # Get reward still missing\n",
    "        reward = get_reward(state, action)\n",
    "\n",
    "        #\n",
    "        q_value = q_values_pred[action]\n",
    "\n",
    "        # Get state still missing\n",
    "        next_state = get_state(state, action)\n",
    "\n",
    "        # Next state == terminal state\n",
    "        if next_state == False:\n",
    "                next_q_value = 0 # No Q-value for terminal\n",
    "        else:\n",
    "                next_q_values = tf.stop_gradient(q_network(next_state)) # No gradient computation\n",
    "                next_action = np.argmax(next_q_values[0])\n",
    "                next_q_value = next_q_values[0, next_action]\n",
    "\n",
    "        observed_q_value = reward + (gamma * next_q_value)\n",
    "\n",
    "        # MSE\n",
    "        loss_value = (observed_q_value-q_value)**2\n",
    "\n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, q_network.trainable_variables)\n",
    "\n",
    "        \"Apply gradients to update network weights\"\n",
    "        optimizer.apply_gradients(zip(grads, q_network.trainable_variables))\n",
    "\n",
    "        \"Update state\"\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7b373b46a3ad398ff0fbe06950ea2901f8c9a237be6643544c903d3b2478057"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('iannwtf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
